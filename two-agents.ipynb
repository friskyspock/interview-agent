{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_questions = [\n",
    "    \"What type of data structure is used here?\",\n",
    "    \"Is candidates approach correct for answer?\",\n",
    "    \"Is candidate using brute force approach here?\",\n",
    "    \"Is candidate using pointers in answer?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from typing import Annotated, TypedDict, Optional\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from pydantic import BaseModel\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    HumanMessage,\n",
    "    ToolMessage,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model=\"llama-3.2-90b-text-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Candidate(BaseModel):\n",
    "    name: str\n",
    "    job_role: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agent(llm, tools, system_message: str):\n",
    "    \"\"\"Create an agent.\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "                \" Use the provided tools to progress towards answering the question.\"\n",
    "                \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "                \" will help where you left off. Execute what you can to make progress.\"\n",
    "                \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "                \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "                \" You have access to the following tools: {tool_names}.\\n{system_message}\",\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ]\n",
    "    )\n",
    "    prompt = prompt.partial(system_message=system_message)\n",
    "    prompt = prompt.partial(tool_names=\", \".join([tool.name for tool in tools]))\n",
    "    return prompt | llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    candidate_name: Optional[str] = None\n",
    "    candidate_role: Optional[str] = None\n",
    "    current_question: Optional[str] = None\n",
    "    current_answer: Optional[str] = None\n",
    "    current_evaluation: Optional[str] = None\n",
    "    history: Optional[str] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greet_user(state: AgentState):\n",
    "    intro = \"Hi, I am Noha, can you please introduce yourself and what you do?\"\n",
    "    print(\"Noha: \", intro)\n",
    "\n",
    "    prompt_extraction = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are an expert extraction algorithm. \"\n",
    "                \"Only extract relevant information from the text. \"\n",
    "                \"If you do not know the value of an attribute asked to extract, \"\n",
    "                \"return null for the attribute's value.\",\n",
    "            ),\n",
    "            (\"human\", \"{text}\"),\n",
    "        ]\n",
    "    )\n",
    "    user_input = input(\"User: \")\n",
    "    runnable = prompt_extraction | llm.with_structured_output(schema=Candidate)\n",
    "    candidate = runnable.invoke({'text': user_input})\n",
    "    return {\"candidate_name\":candidate.name, \"candidate_role\":candidate.job_role}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_main_question() -> dict:\n",
    "    main_question = \"Can you tell me how we can reverse words in string 'I am a boy'?\"\n",
    "    ideal_answer = '''#### Strategy\n",
    "    Two Pointers.\n",
    "\n",
    "    #### Explanation\n",
    "    First, the algorithm splits the words of the sentence from a string into an array.\\\n",
    "    The algorithm also initializes low and high pointers as the first and last indexes of the array.\\\n",
    "    Then, the algorithm iterates the array.\n",
    "\n",
    "    For each iteration, the algorithm swaps the words at the low and high pointers,\\\n",
    "    increments and decrements the low and high pointers respectively,\\\n",
    "    and repeats until the low pointer is greater than the high pointer.\n",
    "\n",
    "    The result is all words of the sentence reversed in the array.\\\n",
    "    The words are joined as a string again with a single space separating the words, and the string is returned.\n",
    "\n",
    "    ##### Time Complexity\n",
    "    The algorithm updates the low and high pointers a number of times proportional to the number of words. \\\n",
    "    Therefore, the time complexity of the algorithm is O(n), where n is the number of words.\n",
    "\n",
    "    ##### Space Complexity\n",
    "    The algorithm splits the words from a sentence into an array. The array is auxiliary space and has length equal to the number of words.\\\n",
    "    Therefore, the auxiliary space complexity of the algorithm is O(n).'''\n",
    "    return {\"question\": main_question, \"answer\": ideal_answer}\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_sub_question(iteration: Annotated[int, \"\"]):\n",
    "    sub_questions = [\n",
    "        \"What type of data structure is used here?\",\n",
    "        \"Is candidates approach correct for answer?\",\n",
    "        \"Is candidate using brute force approach here?\",\n",
    "        \"Is candidate using pointers in answer?\"\n",
    "    ]\n",
    "    return sub_questions[iteration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_1 = create_agent(\n",
    "    llm=llm,\n",
    "    tools=[get_main_question, get_sub_question],\n",
    "    system_message=\"You will ask question to user and evaluate the answer given.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_1():\n",
    "    return {}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
