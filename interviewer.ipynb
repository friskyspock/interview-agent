{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from typing import Annotated, TypedDict, Optional\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model=\"llama-3.2-90b-text-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Candidate(BaseModel):\n",
    "    name: str\n",
    "    job_role: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    history: Optional[str] = None\n",
    "    candidate_name: Optional[str] = None\n",
    "    role: Optional[str] = None\n",
    "    total_questions: Optional[int] = None\n",
    "    question: Optional[str] = None\n",
    "    answer: Optional[str] = None\n",
    "\n",
    "workflow = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_extraction = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert extraction algorithm. \"\n",
    "            \"Only extract relevant information from the text. \"\n",
    "            \"If you do not know the value of an attribute asked to extract, \"\n",
    "            \"return null for the attribute's value.\",\n",
    "        ),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt_question = '''You are interviewer bot. Your name is Noha.\n",
    "You need to interview a candidate for role of {role}.\n",
    "This is a interview so far:\n",
    "{history}\\n\n",
    "Ask your next question and dont repeat any question.'''\n",
    "\n",
    "prompt_response = '''Check whether the answer given for asked question is correct or not.\n",
    "Give a short response on it.\n",
    "question: {question}\n",
    "answer: {answer}'''\n",
    "\n",
    "prompt_verdict = '''How do you feel about the overall interview given below?\n",
    "{history}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduction(state: State):\n",
    "    intro = \"Hi, I am Noha, can you please introduce yourself and what you do?\"\n",
    "    print(\"Noha: \", intro)\n",
    "    question = \"question: \"+intro\n",
    "    return {\"history\":question,\"question\":question,\"total_questions\":1}\n",
    "\n",
    "def handle_intro(state: State):\n",
    "    history = state.get('history', '').strip()\n",
    "    user_input = input(\"User: \")\n",
    "    runnable = prompt_extraction | llm.with_structured_output(schema=Candidate)\n",
    "    candidate = runnable.invoke({'text': user_input})\n",
    "    return {\"history\":history+'\\n'+\"User: \"+user_input,\"candidate_name\":candidate.name, \"role\":candidate.job_role}\n",
    "\n",
    "def handle_question(state: State):\n",
    "    history = state.get('history', '').strip()\n",
    "    role = state.get('role', '').strip()\n",
    "\n",
    "    prompt = prompt_question.format(role=role, history=history)\n",
    "    question = llm.invoke(prompt).content\n",
    "    print(\"Noha: \", question)\n",
    "    return {\"history\":history+'\\n'+question,\"question\":\"question: \"+question,\"total_questions\":state.get('total_questions')+1}\n",
    "\n",
    "def handle_answer(state: State):\n",
    "    history = state.get('history', '').strip()\n",
    "    user_input = input(\"User: \")\n",
    "    return {\"history\":history+'\\n'+\"User: \"+user_input,\"answer\":user_input}\n",
    "\n",
    "def handle_response(state: State):\n",
    "    history = state.get('history', '').strip()\n",
    "    question = state.get('question', '').strip()\n",
    "    answer = state.get('answer', '').strip()\n",
    "\n",
    "    prompt = prompt_response.format(question=question, answer=answer)\n",
    "    response = llm.invoke(prompt).content\n",
    "    print(\"Noha: \", response)\n",
    "    return {\"history\":history+'\\n'+response}\n",
    "\n",
    "def handle_verdict(state: State):\n",
    "    history = state.get('history', '').strip()\n",
    "\n",
    "    prompt = prompt_verdict.format(history=history)\n",
    "    verdict = \"question: \"+llm.invoke(prompt).content\n",
    "    print(\"Noha: \",verdict)\n",
    "    return {\"history\":history+'\\n'+verdict, \"total_questions\":state.get('total_questions')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_conv_length(state: State):\n",
    "    if state.get('total_questions') < 5:\n",
    "        return \"handle_question\"\n",
    "    else:\n",
    "        return \"handle_verdict\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1d1effa4550>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.add_node(\"introduction\", introduction)\n",
    "workflow.add_node(\"handle_intro\", handle_intro)\n",
    "workflow.add_node(\"handle_question\", handle_question)\n",
    "workflow.add_node(\"handle_answer\", handle_answer)\n",
    "workflow.add_node(\"handle_response\", handle_response)\n",
    "workflow.add_node(\"handle_verdict\", handle_verdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1d1effa4550>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.add_conditional_edges(\n",
    "    \"handle_response\",\n",
    "    check_conv_length,\n",
    "    {\"handle_question\":\"handle_question\",\"handle_verdict\":\"handle_verdict\"}\n",
    "    )\n",
    "\n",
    "workflow.add_edge(START, \"introduction\")\n",
    "workflow.add_edge(\"introduction\",\"handle_intro\")\n",
    "workflow.add_edge(\"handle_intro\",\"handle_question\")\n",
    "workflow.add_edge(\"handle_question\",\"handle_answer\")\n",
    "workflow.add_edge(\"handle_answer\",\"handle_response\")\n",
    "workflow.add_edge(\"handle_verdict\",END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_graph_updates(user_input: str):\n",
    "    for event in app.stream({\"answer\": user_input}):\n",
    "        for value in event.values():\n",
    "            print(\"Noha:\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, I am Noha, can you please introduce yourself?\n",
      "{'introduction': {'history': 'question: Hi, I am Noha, can you please introduce yourself?', 'total_questions': 1, 'question': 'question: Hi, I am Noha, can you please introduce yourself?'}}\n",
      "{'handle_intro': {'history': 'question: Hi, I am Noha, can you please introduce yourself?\\nUser: I am aniket, being in the industry for last 4 years as a Data Scientist', 'candidate_name': 'Aniket', 'role': 'Data Scientist'}}\n",
      "{'handle_question': {'history': \"question: Hi, I am Noha, can you please introduce yourself?\\nUser: I am aniket, being in the industry for last 4 years as a Data Scientist\\nquestion: Hello Aniket, nice to meet you. With 4 years of experience as a Data Scientist, I'm sure you have worked on various projects. \\n\\nCan you walk me through one of your most challenging data science projects you've worked on, and how you overcame the obstacles you faced during that project?\", 'total_questions': 2, 'question': \"question: Hello Aniket, nice to meet you. With 4 years of experience as a Data Scientist, I'm sure you have worked on various projects. \\n\\nCan you walk me through one of your most challenging data science projects you've worked on, and how you overcame the obstacles you faced during that project?\"}}\n",
      "{'handle_answer': {'history': \"question: Hi, I am Noha, can you please introduce yourself?\\nUser: I am aniket, being in the industry for last 4 years as a Data Scientist\\nquestion: Hello Aniket, nice to meet you. With 4 years of experience as a Data Scientist, I'm sure you have worked on various projects. \\n\\nCan you walk me through one of your most challenging data science projects you've worked on, and how you overcame the obstacles you faced during that project?\\nUser: I have worked on Sentiment Analysis of tweets which was my best project\", 'answer': 'I have worked on Sentiment Analysis of tweets which was my best project'}}\n",
      "The answer is partially correct but incomplete. The interviewer asked for a detailed explanation of a challenging project, how it was challenging, and how the obstacles were overcome. However, the answer provided only mentions the project (Sentiment Analysis of tweets) without explaining the challenges and how they were overcome. A more detailed and descriptive answer is expected.\n",
      "{'handle_response': {'history': \"question: Hi, I am Noha, can you please introduce yourself?\\nUser: I am aniket, being in the industry for last 4 years as a Data Scientist\\nquestion: Hello Aniket, nice to meet you. With 4 years of experience as a Data Scientist, I'm sure you have worked on various projects. \\n\\nCan you walk me through one of your most challenging data science projects you've worked on, and how you overcame the obstacles you faced during that project?\\nUser: I have worked on Sentiment Analysis of tweets which was my best project\\nThe answer is partially correct but incomplete. The interviewer asked for a detailed explanation of a challenging project, how it was challenging, and how the obstacles were overcome. However, the answer provided only mentions the project (Sentiment Analysis of tweets) without explaining the challenges and how they were overcome. A more detailed and descriptive answer is expected.\"}}\n",
      "{'handle_question': {'history': \"question: Hi, I am Noha, can you please introduce yourself?\\nUser: I am aniket, being in the industry for last 4 years as a Data Scientist\\nquestion: Hello Aniket, nice to meet you. With 4 years of experience as a Data Scientist, I'm sure you have worked on various projects. \\n\\nCan you walk me through one of your most challenging data science projects you've worked on, and how you overcame the obstacles you faced during that project?\\nUser: I have worked on Sentiment Analysis of tweets which was my best project\\nThe answer is partially correct but incomplete. The interviewer asked for a detailed explanation of a challenging project, how it was challenging, and how the obstacles were overcome. However, the answer provided only mentions the project (Sentiment Analysis of tweets) without explaining the challenges and how they were overcome. A more detailed and descriptive answer is expected.\\nquestion: Aniket, it's great that you mentioned the Sentiment Analysis of tweets project, but could you elaborate more on what made this project challenging for you? For instance, what kind of obstacles did you face in terms of data quality, model performance, or interpreting the results? Additionally, can you walk me through the specific steps you took to address these challenges and achieve your desired outcome? \\n\\nAlso, what metrics did you use to measure the success of this project, and how did you evaluate the performance of your sentiment analysis model?\", 'total_questions': 3, 'question': \"question: Aniket, it's great that you mentioned the Sentiment Analysis of tweets project, but could you elaborate more on what made this project challenging for you? For instance, what kind of obstacles did you face in terms of data quality, model performance, or interpreting the results? Additionally, can you walk me through the specific steps you took to address these challenges and achieve your desired outcome? \\n\\nAlso, what metrics did you use to measure the success of this project, and how did you evaluate the performance of your sentiment analysis model?\"}}\n",
      "{'handle_answer': {'history': \"question: Hi, I am Noha, can you please introduce yourself?\\nUser: I am aniket, being in the industry for last 4 years as a Data Scientist\\nquestion: Hello Aniket, nice to meet you. With 4 years of experience as a Data Scientist, I'm sure you have worked on various projects. \\n\\nCan you walk me through one of your most challenging data science projects you've worked on, and how you overcame the obstacles you faced during that project?\\nUser: I have worked on Sentiment Analysis of tweets which was my best project\\nThe answer is partially correct but incomplete. The interviewer asked for a detailed explanation of a challenging project, how it was challenging, and how the obstacles were overcome. However, the answer provided only mentions the project (Sentiment Analysis of tweets) without explaining the challenges and how they were overcome. A more detailed and descriptive answer is expected.\\nquestion: Aniket, it's great that you mentioned the Sentiment Analysis of tweets project, but could you elaborate more on what made this project challenging for you? For instance, what kind of obstacles did you face in terms of data quality, model performance, or interpreting the results? Additionally, can you walk me through the specific steps you took to address these challenges and achieve your desired outcome? \\n\\nAlso, what metrics did you use to measure the success of this project, and how did you evaluate the performance of your sentiment analysis model?\\nUser: It was hard for me to understand the naive bayes algorithm I used. But in the end I was able to achieve 90 percent accuracy on my test data\", 'answer': 'It was hard for me to understand the naive bayes algorithm I used. But in the end I was able to achieve 90 percent accuracy on my test data'}}\n",
      "The answer is partially correct but incomplete. \n",
      "\n",
      "The respondent has identified a challenge (understanding the Naive Bayes algorithm) but has not elaborated on the obstacles they faced in terms of data quality or model performance. They also haven't explained the steps they took to address these challenges.\n",
      "\n",
      "Additionally, they have only mentioned one metric (accuracy) to measure the success of the project. They should have discussed other metrics like precision, recall, and F1-score to evaluate the performance of their sentiment analysis model.\n",
      "{'handle_response': {'history': \"question: Hi, I am Noha, can you please introduce yourself?\\nUser: I am aniket, being in the industry for last 4 years as a Data Scientist\\nquestion: Hello Aniket, nice to meet you. With 4 years of experience as a Data Scientist, I'm sure you have worked on various projects. \\n\\nCan you walk me through one of your most challenging data science projects you've worked on, and how you overcame the obstacles you faced during that project?\\nUser: I have worked on Sentiment Analysis of tweets which was my best project\\nThe answer is partially correct but incomplete. The interviewer asked for a detailed explanation of a challenging project, how it was challenging, and how the obstacles were overcome. However, the answer provided only mentions the project (Sentiment Analysis of tweets) without explaining the challenges and how they were overcome. A more detailed and descriptive answer is expected.\\nquestion: Aniket, it's great that you mentioned the Sentiment Analysis of tweets project, but could you elaborate more on what made this project challenging for you? For instance, what kind of obstacles did you face in terms of data quality, model performance, or interpreting the results? Additionally, can you walk me through the specific steps you took to address these challenges and achieve your desired outcome? \\n\\nAlso, what metrics did you use to measure the success of this project, and how did you evaluate the performance of your sentiment analysis model?\\nUser: It was hard for me to understand the naive bayes algorithm I used. But in the end I was able to achieve 90 percent accuracy on my test data\\nThe answer is partially correct but incomplete. \\n\\nThe respondent has identified a challenge (understanding the Naive Bayes algorithm) but has not elaborated on the obstacles they faced in terms of data quality or model performance. They also haven't explained the steps they took to address these challenges.\\n\\nAdditionally, they have only mentioned one metric (accuracy) to measure the success of the project. They should have discussed other metrics like precision, recall, and F1-score to evaluate the performance of their sentiment analysis model.\"}}\n",
      "{'handle_question': {'history': \"question: Hi, I am Noha, can you please introduce yourself?\\nUser: I am aniket, being in the industry for last 4 years as a Data Scientist\\nquestion: Hello Aniket, nice to meet you. With 4 years of experience as a Data Scientist, I'm sure you have worked on various projects. \\n\\nCan you walk me through one of your most challenging data science projects you've worked on, and how you overcame the obstacles you faced during that project?\\nUser: I have worked on Sentiment Analysis of tweets which was my best project\\nThe answer is partially correct but incomplete. The interviewer asked for a detailed explanation of a challenging project, how it was challenging, and how the obstacles were overcome. However, the answer provided only mentions the project (Sentiment Analysis of tweets) without explaining the challenges and how they were overcome. A more detailed and descriptive answer is expected.\\nquestion: Aniket, it's great that you mentioned the Sentiment Analysis of tweets project, but could you elaborate more on what made this project challenging for you? For instance, what kind of obstacles did you face in terms of data quality, model performance, or interpreting the results? Additionally, can you walk me through the specific steps you took to address these challenges and achieve your desired outcome? \\n\\nAlso, what metrics did you use to measure the success of this project, and how did you evaluate the performance of your sentiment analysis model?\\nUser: It was hard for me to understand the naive bayes algorithm I used. But in the end I was able to achieve 90 percent accuracy on my test data\\nThe answer is partially correct but incomplete. \\n\\nThe respondent has identified a challenge (understanding the Naive Bayes algorithm) but has not elaborated on the obstacles they faced in terms of data quality or model performance. They also haven't explained the steps they took to address these challenges.\\n\\nAdditionally, they have only mentioned one metric (accuracy) to measure the success of the project. They should have discussed other metrics like precision, recall, and F1-score to evaluate the performance of their sentiment analysis model.\\nquestion: Aniket, I appreciate you sharing the challenge you faced with understanding the Naive Bayes algorithm and the accuracy you achieved. However, I'd like to explore more about the data preprocessing steps you took for this project.\\n\\nCan you walk me through how you handled the preprocessing of the Twitter data, such as handling missing values, removing stop words, stemming or lemmatizing words, and dealing with class imbalance? Also, did you use any techniques like word embeddings (e.g., Word2Vec, GloVe) to represent the text data, and if so, how did you incorporate them into your model?\", 'total_questions': 4, 'question': \"question: Aniket, I appreciate you sharing the challenge you faced with understanding the Naive Bayes algorithm and the accuracy you achieved. However, I'd like to explore more about the data preprocessing steps you took for this project.\\n\\nCan you walk me through how you handled the preprocessing of the Twitter data, such as handling missing values, removing stop words, stemming or lemmatizing words, and dealing with class imbalance? Also, did you use any techniques like word embeddings (e.g., Word2Vec, GloVe) to represent the text data, and if so, how did you incorporate them into your model?\"}}\n",
      "{'handle_answer': {'history': \"question: Hi, I am Noha, can you please introduce yourself?\\nUser: I am aniket, being in the industry for last 4 years as a Data Scientist\\nquestion: Hello Aniket, nice to meet you. With 4 years of experience as a Data Scientist, I'm sure you have worked on various projects. \\n\\nCan you walk me through one of your most challenging data science projects you've worked on, and how you overcame the obstacles you faced during that project?\\nUser: I have worked on Sentiment Analysis of tweets which was my best project\\nThe answer is partially correct but incomplete. The interviewer asked for a detailed explanation of a challenging project, how it was challenging, and how the obstacles were overcome. However, the answer provided only mentions the project (Sentiment Analysis of tweets) without explaining the challenges and how they were overcome. A more detailed and descriptive answer is expected.\\nquestion: Aniket, it's great that you mentioned the Sentiment Analysis of tweets project, but could you elaborate more on what made this project challenging for you? For instance, what kind of obstacles did you face in terms of data quality, model performance, or interpreting the results? Additionally, can you walk me through the specific steps you took to address these challenges and achieve your desired outcome? \\n\\nAlso, what metrics did you use to measure the success of this project, and how did you evaluate the performance of your sentiment analysis model?\\nUser: It was hard for me to understand the naive bayes algorithm I used. But in the end I was able to achieve 90 percent accuracy on my test data\\nThe answer is partially correct but incomplete. \\n\\nThe respondent has identified a challenge (understanding the Naive Bayes algorithm) but has not elaborated on the obstacles they faced in terms of data quality or model performance. They also haven't explained the steps they took to address these challenges.\\n\\nAdditionally, they have only mentioned one metric (accuracy) to measure the success of the project. They should have discussed other metrics like precision, recall, and F1-score to evaluate the performance of their sentiment analysis model.\\nquestion: Aniket, I appreciate you sharing the challenge you faced with understanding the Naive Bayes algorithm and the accuracy you achieved. However, I'd like to explore more about the data preprocessing steps you took for this project.\\n\\nCan you walk me through how you handled the preprocessing of the Twitter data, such as handling missing values, removing stop words, stemming or lemmatizing words, and dealing with class imbalance? Also, did you use any techniques like word embeddings (e.g., Word2Vec, GloVe) to represent the text data, and if so, how did you incorporate them into your model?\\nUser: I removed stopwords using nltk library, I did lemmatization and used word2vec for embeddings\", 'answer': 'I removed stopwords using nltk library, I did lemmatization and used word2vec for embeddings'}}\n",
      "The answer is partially correct. The user has mentioned that they:\n",
      "\n",
      "1. Removed stop words using NLTK library (Correct)\n",
      "2. Performed lemmatization (Correct)\n",
      "3. Used Word2Vec for word embeddings (Correct)\n",
      "\n",
      "However, the answer lacks details on other preprocessing steps such as:\n",
      "\n",
      "* Handling missing values\n",
      "* Dealing with class imbalance\n",
      "* How they incorporated Word2Vec embeddings into their model\n",
      "{'handle_response': {'history': \"question: Hi, I am Noha, can you please introduce yourself?\\nUser: I am aniket, being in the industry for last 4 years as a Data Scientist\\nquestion: Hello Aniket, nice to meet you. With 4 years of experience as a Data Scientist, I'm sure you have worked on various projects. \\n\\nCan you walk me through one of your most challenging data science projects you've worked on, and how you overcame the obstacles you faced during that project?\\nUser: I have worked on Sentiment Analysis of tweets which was my best project\\nThe answer is partially correct but incomplete. The interviewer asked for a detailed explanation of a challenging project, how it was challenging, and how the obstacles were overcome. However, the answer provided only mentions the project (Sentiment Analysis of tweets) without explaining the challenges and how they were overcome. A more detailed and descriptive answer is expected.\\nquestion: Aniket, it's great that you mentioned the Sentiment Analysis of tweets project, but could you elaborate more on what made this project challenging for you? For instance, what kind of obstacles did you face in terms of data quality, model performance, or interpreting the results? Additionally, can you walk me through the specific steps you took to address these challenges and achieve your desired outcome? \\n\\nAlso, what metrics did you use to measure the success of this project, and how did you evaluate the performance of your sentiment analysis model?\\nUser: It was hard for me to understand the naive bayes algorithm I used. But in the end I was able to achieve 90 percent accuracy on my test data\\nThe answer is partially correct but incomplete. \\n\\nThe respondent has identified a challenge (understanding the Naive Bayes algorithm) but has not elaborated on the obstacles they faced in terms of data quality or model performance. They also haven't explained the steps they took to address these challenges.\\n\\nAdditionally, they have only mentioned one metric (accuracy) to measure the success of the project. They should have discussed other metrics like precision, recall, and F1-score to evaluate the performance of their sentiment analysis model.\\nquestion: Aniket, I appreciate you sharing the challenge you faced with understanding the Naive Bayes algorithm and the accuracy you achieved. However, I'd like to explore more about the data preprocessing steps you took for this project.\\n\\nCan you walk me through how you handled the preprocessing of the Twitter data, such as handling missing values, removing stop words, stemming or lemmatizing words, and dealing with class imbalance? Also, did you use any techniques like word embeddings (e.g., Word2Vec, GloVe) to represent the text data, and if so, how did you incorporate them into your model?\\nUser: I removed stopwords using nltk library, I did lemmatization and used word2vec for embeddings\\nThe answer is partially correct. The user has mentioned that they:\\n\\n1. Removed stop words using NLTK library (Correct)\\n2. Performed lemmatization (Correct)\\n3. Used Word2Vec for word embeddings (Correct)\\n\\nHowever, the answer lacks details on other preprocessing steps such as:\\n\\n* Handling missing values\\n* Dealing with class imbalance\\n* How they incorporated Word2Vec embeddings into their model\"}}\n",
      "{'handle_question': {'history': \"question: Hi, I am Noha, can you please introduce yourself?\\nUser: I am aniket, being in the industry for last 4 years as a Data Scientist\\nquestion: Hello Aniket, nice to meet you. With 4 years of experience as a Data Scientist, I'm sure you have worked on various projects. \\n\\nCan you walk me through one of your most challenging data science projects you've worked on, and how you overcame the obstacles you faced during that project?\\nUser: I have worked on Sentiment Analysis of tweets which was my best project\\nThe answer is partially correct but incomplete. The interviewer asked for a detailed explanation of a challenging project, how it was challenging, and how the obstacles were overcome. However, the answer provided only mentions the project (Sentiment Analysis of tweets) without explaining the challenges and how they were overcome. A more detailed and descriptive answer is expected.\\nquestion: Aniket, it's great that you mentioned the Sentiment Analysis of tweets project, but could you elaborate more on what made this project challenging for you? For instance, what kind of obstacles did you face in terms of data quality, model performance, or interpreting the results? Additionally, can you walk me through the specific steps you took to address these challenges and achieve your desired outcome? \\n\\nAlso, what metrics did you use to measure the success of this project, and how did you evaluate the performance of your sentiment analysis model?\\nUser: It was hard for me to understand the naive bayes algorithm I used. But in the end I was able to achieve 90 percent accuracy on my test data\\nThe answer is partially correct but incomplete. \\n\\nThe respondent has identified a challenge (understanding the Naive Bayes algorithm) but has not elaborated on the obstacles they faced in terms of data quality or model performance. They also haven't explained the steps they took to address these challenges.\\n\\nAdditionally, they have only mentioned one metric (accuracy) to measure the success of the project. They should have discussed other metrics like precision, recall, and F1-score to evaluate the performance of their sentiment analysis model.\\nquestion: Aniket, I appreciate you sharing the challenge you faced with understanding the Naive Bayes algorithm and the accuracy you achieved. However, I'd like to explore more about the data preprocessing steps you took for this project.\\n\\nCan you walk me through how you handled the preprocessing of the Twitter data, such as handling missing values, removing stop words, stemming or lemmatizing words, and dealing with class imbalance? Also, did you use any techniques like word embeddings (e.g., Word2Vec, GloVe) to represent the text data, and if so, how did you incorporate them into your model?\\nUser: I removed stopwords using nltk library, I did lemmatization and used word2vec for embeddings\\nThe answer is partially correct. The user has mentioned that they:\\n\\n1. Removed stop words using NLTK library (Correct)\\n2. Performed lemmatization (Correct)\\n3. Used Word2Vec for word embeddings (Correct)\\n\\nHowever, the answer lacks details on other preprocessing steps such as:\\n\\n* Handling missing values\\n* Dealing with class imbalance\\n* How they incorporated Word2Vec embeddings into their model\\nquestion: Aniket, it's great that you've mentioned some of the preprocessing steps you took for this project, such as removing stop words, lemmatization, and using Word2Vec for word embeddings. However, I'd like to know more about how you handled the class imbalance issue in your sentiment analysis dataset.\\n\\nCan you explain how you identified the class imbalance problem, and what techniques you used to address it, such as oversampling the minority class, undersampling the majority class, or using class-weighting techniques? Additionally, how did you evaluate the effectiveness of these techniques in improving the performance of your sentiment analysis model?\", 'total_questions': 5, 'question': \"question: Aniket, it's great that you've mentioned some of the preprocessing steps you took for this project, such as removing stop words, lemmatization, and using Word2Vec for word embeddings. However, I'd like to know more about how you handled the class imbalance issue in your sentiment analysis dataset.\\n\\nCan you explain how you identified the class imbalance problem, and what techniques you used to address it, such as oversampling the minority class, undersampling the majority class, or using class-weighting techniques? Additionally, how did you evaluate the effectiveness of these techniques in improving the performance of your sentiment analysis model?\"}}\n",
      "{'handle_answer': {'history': \"question: Hi, I am Noha, can you please introduce yourself?\\nUser: I am aniket, being in the industry for last 4 years as a Data Scientist\\nquestion: Hello Aniket, nice to meet you. With 4 years of experience as a Data Scientist, I'm sure you have worked on various projects. \\n\\nCan you walk me through one of your most challenging data science projects you've worked on, and how you overcame the obstacles you faced during that project?\\nUser: I have worked on Sentiment Analysis of tweets which was my best project\\nThe answer is partially correct but incomplete. The interviewer asked for a detailed explanation of a challenging project, how it was challenging, and how the obstacles were overcome. However, the answer provided only mentions the project (Sentiment Analysis of tweets) without explaining the challenges and how they were overcome. A more detailed and descriptive answer is expected.\\nquestion: Aniket, it's great that you mentioned the Sentiment Analysis of tweets project, but could you elaborate more on what made this project challenging for you? For instance, what kind of obstacles did you face in terms of data quality, model performance, or interpreting the results? Additionally, can you walk me through the specific steps you took to address these challenges and achieve your desired outcome? \\n\\nAlso, what metrics did you use to measure the success of this project, and how did you evaluate the performance of your sentiment analysis model?\\nUser: It was hard for me to understand the naive bayes algorithm I used. But in the end I was able to achieve 90 percent accuracy on my test data\\nThe answer is partially correct but incomplete. \\n\\nThe respondent has identified a challenge (understanding the Naive Bayes algorithm) but has not elaborated on the obstacles they faced in terms of data quality or model performance. They also haven't explained the steps they took to address these challenges.\\n\\nAdditionally, they have only mentioned one metric (accuracy) to measure the success of the project. They should have discussed other metrics like precision, recall, and F1-score to evaluate the performance of their sentiment analysis model.\\nquestion: Aniket, I appreciate you sharing the challenge you faced with understanding the Naive Bayes algorithm and the accuracy you achieved. However, I'd like to explore more about the data preprocessing steps you took for this project.\\n\\nCan you walk me through how you handled the preprocessing of the Twitter data, such as handling missing values, removing stop words, stemming or lemmatizing words, and dealing with class imbalance? Also, did you use any techniques like word embeddings (e.g., Word2Vec, GloVe) to represent the text data, and if so, how did you incorporate them into your model?\\nUser: I removed stopwords using nltk library, I did lemmatization and used word2vec for embeddings\\nThe answer is partially correct. The user has mentioned that they:\\n\\n1. Removed stop words using NLTK library (Correct)\\n2. Performed lemmatization (Correct)\\n3. Used Word2Vec for word embeddings (Correct)\\n\\nHowever, the answer lacks details on other preprocessing steps such as:\\n\\n* Handling missing values\\n* Dealing with class imbalance\\n* How they incorporated Word2Vec embeddings into their model\\nquestion: Aniket, it's great that you've mentioned some of the preprocessing steps you took for this project, such as removing stop words, lemmatization, and using Word2Vec for word embeddings. However, I'd like to know more about how you handled the class imbalance issue in your sentiment analysis dataset.\\n\\nCan you explain how you identified the class imbalance problem, and what techniques you used to address it, such as oversampling the minority class, undersampling the majority class, or using class-weighting techniques? Additionally, how did you evaluate the effectiveness of these techniques in improving the performance of your sentiment analysis model?\\nUser: I dont know how to do that\", 'answer': 'I dont know how to do that'}}\n",
      "The answer is incorrect. It does not address the question about class imbalance in the sentiment analysis dataset, and instead responds with a lack of knowledge on the topic. A correct answer would provide a clear explanation of how the class imbalance was identified and handled, including the techniques used to address it and how their effectiveness was evaluated.\n",
      "{'handle_response': {'history': \"question: Hi, I am Noha, can you please introduce yourself?\\nUser: I am aniket, being in the industry for last 4 years as a Data Scientist\\nquestion: Hello Aniket, nice to meet you. With 4 years of experience as a Data Scientist, I'm sure you have worked on various projects. \\n\\nCan you walk me through one of your most challenging data science projects you've worked on, and how you overcame the obstacles you faced during that project?\\nUser: I have worked on Sentiment Analysis of tweets which was my best project\\nThe answer is partially correct but incomplete. The interviewer asked for a detailed explanation of a challenging project, how it was challenging, and how the obstacles were overcome. However, the answer provided only mentions the project (Sentiment Analysis of tweets) without explaining the challenges and how they were overcome. A more detailed and descriptive answer is expected.\\nquestion: Aniket, it's great that you mentioned the Sentiment Analysis of tweets project, but could you elaborate more on what made this project challenging for you? For instance, what kind of obstacles did you face in terms of data quality, model performance, or interpreting the results? Additionally, can you walk me through the specific steps you took to address these challenges and achieve your desired outcome? \\n\\nAlso, what metrics did you use to measure the success of this project, and how did you evaluate the performance of your sentiment analysis model?\\nUser: It was hard for me to understand the naive bayes algorithm I used. But in the end I was able to achieve 90 percent accuracy on my test data\\nThe answer is partially correct but incomplete. \\n\\nThe respondent has identified a challenge (understanding the Naive Bayes algorithm) but has not elaborated on the obstacles they faced in terms of data quality or model performance. They also haven't explained the steps they took to address these challenges.\\n\\nAdditionally, they have only mentioned one metric (accuracy) to measure the success of the project. They should have discussed other metrics like precision, recall, and F1-score to evaluate the performance of their sentiment analysis model.\\nquestion: Aniket, I appreciate you sharing the challenge you faced with understanding the Naive Bayes algorithm and the accuracy you achieved. However, I'd like to explore more about the data preprocessing steps you took for this project.\\n\\nCan you walk me through how you handled the preprocessing of the Twitter data, such as handling missing values, removing stop words, stemming or lemmatizing words, and dealing with class imbalance? Also, did you use any techniques like word embeddings (e.g., Word2Vec, GloVe) to represent the text data, and if so, how did you incorporate them into your model?\\nUser: I removed stopwords using nltk library, I did lemmatization and used word2vec for embeddings\\nThe answer is partially correct. The user has mentioned that they:\\n\\n1. Removed stop words using NLTK library (Correct)\\n2. Performed lemmatization (Correct)\\n3. Used Word2Vec for word embeddings (Correct)\\n\\nHowever, the answer lacks details on other preprocessing steps such as:\\n\\n* Handling missing values\\n* Dealing with class imbalance\\n* How they incorporated Word2Vec embeddings into their model\\nquestion: Aniket, it's great that you've mentioned some of the preprocessing steps you took for this project, such as removing stop words, lemmatization, and using Word2Vec for word embeddings. However, I'd like to know more about how you handled the class imbalance issue in your sentiment analysis dataset.\\n\\nCan you explain how you identified the class imbalance problem, and what techniques you used to address it, such as oversampling the minority class, undersampling the majority class, or using class-weighting techniques? Additionally, how did you evaluate the effectiveness of these techniques in improving the performance of your sentiment analysis model?\\nUser: I dont know how to do that\\nThe answer is incorrect. It does not address the question about class imbalance in the sentiment analysis dataset, and instead responds with a lack of knowledge on the topic. A correct answer would provide a clear explanation of how the class imbalance was identified and handled, including the techniques used to address it and how their effectiveness was evaluated.\"}}\n"
     ]
    },
    {
     "ename": "InvalidUpdateError",
     "evalue": "Must write to at least one of ['history', 'candidate_name', 'role', 'total_questions', 'question', 'answer']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidUpdateError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m app\u001b[38;5;241m.\u001b[39mstream({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhistory\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcandidate_name\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_questions\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m0\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m}):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(event)\n",
      "File \u001b[1;32md:\\Noha.ai\\.venv\\lib\\site-packages\\langgraph\\pregel\\__init__.py:1315\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[0;32m   1305\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[0;32m   1308\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[0;32m   1309\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(\n\u001b[0;32m   1310\u001b[0m         input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels,\n\u001b[0;32m   1311\u001b[0m         interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before_,\n\u001b[0;32m   1312\u001b[0m         interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after_,\n\u001b[0;32m   1313\u001b[0m         manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m   1314\u001b[0m     ):\n\u001b[1;32m-> 1315\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[0;32m   1316\u001b[0m             loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m   1317\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[0;32m   1318\u001b[0m             retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[0;32m   1319\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[0;32m   1320\u001b[0m         ):\n\u001b[0;32m   1321\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[0;32m   1322\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[0;32m   1323\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[1;32md:\\Noha.ai\\.venv\\lib\\site-packages\\langgraph\\pregel\\runner.py:56\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[0;32m     54\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 56\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32md:\\Noha.ai\\.venv\\lib\\site-packages\\langgraph\\pregel\\retry.py:29\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy)\u001b[0m\n\u001b[0;32m     27\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32md:\\Noha.ai\\.venv\\lib\\site-packages\\langgraph\\utils\\runnable.py:412\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    410\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    411\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 412\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Noha.ai\\.venv\\lib\\site-packages\\langgraph\\utils\\runnable.py:176\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m    175\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m--> 176\u001b[0m     ret \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    178\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32md:\\Noha.ai\\.venv\\lib\\site-packages\\langgraph\\pregel\\write.py:85\u001b[0m, in \u001b[0;36mChannelWrite._write\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_write\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Any, config: RunnableConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m     writes \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     80\u001b[0m         ChannelWriteEntry(write\u001b[38;5;241m.\u001b[39mchannel, \u001b[38;5;28minput\u001b[39m, write\u001b[38;5;241m.\u001b[39mskip_none, write\u001b[38;5;241m.\u001b[39mmapper)\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(write, ChannelWriteEntry) \u001b[38;5;129;01mand\u001b[39;00m write\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mis\u001b[39;00m PASSTHROUGH\n\u001b[0;32m     82\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m write\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m write \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrites\n\u001b[0;32m     84\u001b[0m     ]\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_write\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_at_least_one_of\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32md:\\Noha.ai\\.venv\\lib\\site-packages\\langgraph\\pregel\\write.py:138\u001b[0m, in \u001b[0;36mChannelWrite.do_write\u001b[1;34m(config, writes, require_at_least_one_of)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m require_at_least_one_of \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m {chan \u001b[38;5;28;01mfor\u001b[39;00m chan, _ \u001b[38;5;129;01min\u001b[39;00m filtered} \u001b[38;5;241m&\u001b[39m \u001b[38;5;28mset\u001b[39m(require_at_least_one_of):\n\u001b[1;32m--> 138\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidUpdateError(\n\u001b[0;32m    139\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust write to at least one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequire_at_least_one_of\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    140\u001b[0m         )\n\u001b[0;32m    141\u001b[0m write: TYPE_SEND \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_SEND]\n\u001b[0;32m    142\u001b[0m write(sends \u001b[38;5;241m+\u001b[39m filtered)\n",
      "\u001b[1;31mInvalidUpdateError\u001b[0m: Must write to at least one of ['history', 'candidate_name', 'role', 'total_questions', 'question', 'answer']"
     ]
    }
   ],
   "source": [
    "for event in app.stream({\"history\":'',\"candidate_name\":'',\"role\":'',\"total_questions\":0,\"question\":'',\"answer\":''}):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = input(\"User: \")\n",
    "\n",
    "app.update_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, I am Noha, can you please introduce yourself?\n",
      "Noha: {'history': 'question: Hi, I am Noha, can you please introduce yourself?', 'total_questions': 1, 'question': 'question: Hi, I am Noha, can you please introduce yourself?'}\n",
      "Noha: {'history': \"question: Hi, I am Noha, can you please introduce yourself?\\nquestion: It's nice to meet you. Can you tell me a little bit about your background and how you think your skills and experience make you a strong fit for this role?\", 'total_questions': 2, 'question': \"question: It's nice to meet you. Can you tell me a little bit about your background and how you think your skills and experience make you a strong fit for this role?\"}\n",
      "The given answer \"Hello\" is incorrect. The correct answer should provide some information about the person's background, skills, and experience related to the role they're applying for.\n",
      "Noha: {'history': 'question: Hi, I am Noha, can you please introduce yourself?\\nquestion: It\\'s nice to meet you. Can you tell me a little bit about your background and how you think your skills and experience make you a strong fit for this role?\\nThe given answer \"Hello\" is incorrect. The correct answer should provide some information about the person\\'s background, skills, and experience related to the role they\\'re applying for.'}\n",
      "Noha: {'history': 'question: Hi, I am Noha, can you please introduce yourself?\\nquestion: It\\'s nice to meet you. Can you tell me a little bit about your background and how you think your skills and experience make you a strong fit for this role?\\nThe given answer \"Hello\" is incorrect. The correct answer should provide some information about the person\\'s background, skills, and experience related to the role they\\'re applying for.\\nquestion: Thank you for sharing that with me. Now, I\\'d like to dive a bit deeper into your previous experience. Can you walk me through a specific project or accomplishment that you\\'re particularly proud of, and how you think it demonstrates your skills and abilities relevant to this role?', 'total_questions': 3, 'question': \"question: Thank you for sharing that with me. Now, I'd like to dive a bit deeper into your previous experience. Can you walk me through a specific project or accomplishment that you're particularly proud of, and how you think it demonstrates your skills and abilities relevant to this role?\"}\n",
      "The answer is incorrect. The response should be a detailed explanation of a specific project or accomplishment, highlighting the skills and abilities relevant to the role, rather than a simple greeting.\n",
      "Noha: {'history': 'question: Hi, I am Noha, can you please introduce yourself?\\nquestion: It\\'s nice to meet you. Can you tell me a little bit about your background and how you think your skills and experience make you a strong fit for this role?\\nThe given answer \"Hello\" is incorrect. The correct answer should provide some information about the person\\'s background, skills, and experience related to the role they\\'re applying for.\\nquestion: Thank you for sharing that with me. Now, I\\'d like to dive a bit deeper into your previous experience. Can you walk me through a specific project or accomplishment that you\\'re particularly proud of, and how you think it demonstrates your skills and abilities relevant to this role?\\nThe answer is incorrect. The response should be a detailed explanation of a specific project or accomplishment, highlighting the skills and abilities relevant to the role, rather than a simple greeting.'}\n",
      "Noha: {'history': 'question: Hi, I am Noha, can you please introduce yourself?\\nquestion: It\\'s nice to meet you. Can you tell me a little bit about your background and how you think your skills and experience make you a strong fit for this role?\\nThe given answer \"Hello\" is incorrect. The correct answer should provide some information about the person\\'s background, skills, and experience related to the role they\\'re applying for.\\nquestion: Thank you for sharing that with me. Now, I\\'d like to dive a bit deeper into your previous experience. Can you walk me through a specific project or accomplishment that you\\'re particularly proud of, and how you think it demonstrates your skills and abilities relevant to this role?\\nThe answer is incorrect. The response should be a detailed explanation of a specific project or accomplishment, highlighting the skills and abilities relevant to the role, rather than a simple greeting.\\nquestion: I\\'m Noha, nice to continue our conversation.  Can you tell me, what are some of the most significant challenges you\\'ve faced in your previous roles, and how did you overcome them?', 'total_questions': 4, 'question': \"question: I'm Noha, nice to continue our conversation.  Can you tell me, what are some of the most significant challenges you've faced in your previous roles, and how did you overcome them?\"}\n",
      "The answer given is incorrect. It does not address the question about significant challenges faced in previous roles and how they were overcome. The response only greets the person, but does not provide any relevant information.\n",
      "Noha: {'history': 'question: Hi, I am Noha, can you please introduce yourself?\\nquestion: It\\'s nice to meet you. Can you tell me a little bit about your background and how you think your skills and experience make you a strong fit for this role?\\nThe given answer \"Hello\" is incorrect. The correct answer should provide some information about the person\\'s background, skills, and experience related to the role they\\'re applying for.\\nquestion: Thank you for sharing that with me. Now, I\\'d like to dive a bit deeper into your previous experience. Can you walk me through a specific project or accomplishment that you\\'re particularly proud of, and how you think it demonstrates your skills and abilities relevant to this role?\\nThe answer is incorrect. The response should be a detailed explanation of a specific project or accomplishment, highlighting the skills and abilities relevant to the role, rather than a simple greeting.\\nquestion: I\\'m Noha, nice to continue our conversation.  Can you tell me, what are some of the most significant challenges you\\'ve faced in your previous roles, and how did you overcome them?\\nThe answer given is incorrect. It does not address the question about significant challenges faced in previous roles and how they were overcome. The response only greets the person, but does not provide any relevant information.'}\n",
      "Noha: {'history': 'question: Hi, I am Noha, can you please introduce yourself?\\nquestion: It\\'s nice to meet you. Can you tell me a little bit about your background and how you think your skills and experience make you a strong fit for this role?\\nThe given answer \"Hello\" is incorrect. The correct answer should provide some information about the person\\'s background, skills, and experience related to the role they\\'re applying for.\\nquestion: Thank you for sharing that with me. Now, I\\'d like to dive a bit deeper into your previous experience. Can you walk me through a specific project or accomplishment that you\\'re particularly proud of, and how you think it demonstrates your skills and abilities relevant to this role?\\nThe answer is incorrect. The response should be a detailed explanation of a specific project or accomplishment, highlighting the skills and abilities relevant to the role, rather than a simple greeting.\\nquestion: I\\'m Noha, nice to continue our conversation.  Can you tell me, what are some of the most significant challenges you\\'ve faced in your previous roles, and how did you overcome them?\\nThe answer given is incorrect. It does not address the question about significant challenges faced in previous roles and how they were overcome. The response only greets the person, but does not provide any relevant information.\\nquestion: I\\'m Noha, thank you for continuing our conversation.  Now, I\\'d like to know more about your approach to problem-solving.  Can you describe your thought process when faced with a complex problem, and how you evaluate potential solutions to determine the best course of action?', 'total_questions': 5, 'question': \"question: I'm Noha, thank you for continuing our conversation.  Now, I'd like to know more about your approach to problem-solving.  Can you describe your thought process when faced with a complex problem, and how you evaluate potential solutions to determine the best course of action?\"}\n",
      "The answer is incorrect. The response is a simple greeting and does not provide any information about the thought process or approach to problem-solving. It does not address the question asked.\n",
      "Noha: {'history': 'question: Hi, I am Noha, can you please introduce yourself?\\nquestion: It\\'s nice to meet you. Can you tell me a little bit about your background and how you think your skills and experience make you a strong fit for this role?\\nThe given answer \"Hello\" is incorrect. The correct answer should provide some information about the person\\'s background, skills, and experience related to the role they\\'re applying for.\\nquestion: Thank you for sharing that with me. Now, I\\'d like to dive a bit deeper into your previous experience. Can you walk me through a specific project or accomplishment that you\\'re particularly proud of, and how you think it demonstrates your skills and abilities relevant to this role?\\nThe answer is incorrect. The response should be a detailed explanation of a specific project or accomplishment, highlighting the skills and abilities relevant to the role, rather than a simple greeting.\\nquestion: I\\'m Noha, nice to continue our conversation.  Can you tell me, what are some of the most significant challenges you\\'ve faced in your previous roles, and how did you overcome them?\\nThe answer given is incorrect. It does not address the question about significant challenges faced in previous roles and how they were overcome. The response only greets the person, but does not provide any relevant information.\\nquestion: I\\'m Noha, thank you for continuing our conversation.  Now, I\\'d like to know more about your approach to problem-solving.  Can you describe your thought process when faced with a complex problem, and how you evaluate potential solutions to determine the best course of action?\\nThe answer is incorrect. The response is a simple greeting and does not provide any information about the thought process or approach to problem-solving. It does not address the question asked.'}\n"
     ]
    },
    {
     "ename": "InvalidUpdateError",
     "evalue": "Must write to at least one of ['history', 'candidate_name', 'role', 'total_questions', 'question', 'answer']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidUpdateError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGoodbye!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mstream_graph_updates\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m, in \u001b[0;36mstream_graph_updates\u001b[1;34m(user_input)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream_graph_updates\u001b[39m(user_input: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m app\u001b[38;5;241m.\u001b[39mstream({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_input}):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m event\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m      4\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNoha:\u001b[39m\u001b[38;5;124m\"\u001b[39m, value)\n",
      "File \u001b[1;32md:\\Noha.ai\\.venv\\lib\\site-packages\\langgraph\\pregel\\__init__.py:1315\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[0;32m   1305\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[0;32m   1308\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[0;32m   1309\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(\n\u001b[0;32m   1310\u001b[0m         input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels,\n\u001b[0;32m   1311\u001b[0m         interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before_,\n\u001b[0;32m   1312\u001b[0m         interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after_,\n\u001b[0;32m   1313\u001b[0m         manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m   1314\u001b[0m     ):\n\u001b[1;32m-> 1315\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[0;32m   1316\u001b[0m             loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m   1317\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[0;32m   1318\u001b[0m             retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[0;32m   1319\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[0;32m   1320\u001b[0m         ):\n\u001b[0;32m   1321\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[0;32m   1322\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[0;32m   1323\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[1;32md:\\Noha.ai\\.venv\\lib\\site-packages\\langgraph\\pregel\\runner.py:56\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[0;32m     54\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 56\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32md:\\Noha.ai\\.venv\\lib\\site-packages\\langgraph\\pregel\\retry.py:29\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy)\u001b[0m\n\u001b[0;32m     27\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32md:\\Noha.ai\\.venv\\lib\\site-packages\\langgraph\\utils\\runnable.py:412\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    410\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    411\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 412\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Noha.ai\\.venv\\lib\\site-packages\\langgraph\\utils\\runnable.py:176\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m    175\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m--> 176\u001b[0m     ret \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    178\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32md:\\Noha.ai\\.venv\\lib\\site-packages\\langgraph\\pregel\\write.py:85\u001b[0m, in \u001b[0;36mChannelWrite._write\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_write\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Any, config: RunnableConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m     writes \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     80\u001b[0m         ChannelWriteEntry(write\u001b[38;5;241m.\u001b[39mchannel, \u001b[38;5;28minput\u001b[39m, write\u001b[38;5;241m.\u001b[39mskip_none, write\u001b[38;5;241m.\u001b[39mmapper)\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(write, ChannelWriteEntry) \u001b[38;5;129;01mand\u001b[39;00m write\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mis\u001b[39;00m PASSTHROUGH\n\u001b[0;32m     82\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m write\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m write \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrites\n\u001b[0;32m     84\u001b[0m     ]\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_write\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_at_least_one_of\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32md:\\Noha.ai\\.venv\\lib\\site-packages\\langgraph\\pregel\\write.py:138\u001b[0m, in \u001b[0;36mChannelWrite.do_write\u001b[1;34m(config, writes, require_at_least_one_of)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m require_at_least_one_of \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m {chan \u001b[38;5;28;01mfor\u001b[39;00m chan, _ \u001b[38;5;129;01min\u001b[39;00m filtered} \u001b[38;5;241m&\u001b[39m \u001b[38;5;28mset\u001b[39m(require_at_least_one_of):\n\u001b[1;32m--> 138\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidUpdateError(\n\u001b[0;32m    139\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust write to at least one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequire_at_least_one_of\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    140\u001b[0m         )\n\u001b[0;32m    141\u001b[0m write: TYPE_SEND \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_SEND]\n\u001b[0;32m    142\u001b[0m write(sends \u001b[38;5;241m+\u001b[39m filtered)\n",
      "\u001b[1;31mInvalidUpdateError\u001b[0m: Must write to at least one of ['history', 'candidate_name', 'role', 'total_questions', 'question', 'answer']"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    \n",
    "    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "\n",
    "    stream_graph_updates(user_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
